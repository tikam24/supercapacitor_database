In this pilot study, we describe self-learning energy management principles for energy harvesting environmental monitoring nodes using Internet of Things (IoT) communications technology. The solution is powered with ambient energy harvested by a thermoelectric generator (TEG) and stored in an internal supercapacitor. We present a hardware-based model derived from a DC/DC converter, microcontroller and LoRaWAN IoT interface, which is detailed in the paper. The simulation applied historical temperature data obtained at several soil depths. The study's contribution is a reinforcement learning (Q-learning) method to achieve an optimal energy management strategy to maximize data collection and minimize failure. The results demonstrate that the designed approach was capable of operating more effectively (up to approx. 96 % ratio between complete and missed cycles) than reference solutions with a fixed duty-cycle configuration. We support our conclusions with results from 10 candidate Q-learning controllers which apply various learning and discount factor configurations and demonstrate superior complete/missed cycles ratios than the reference solutions.